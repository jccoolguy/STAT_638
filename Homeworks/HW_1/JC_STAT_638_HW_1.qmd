---
title: "Homework 1"
author: "Jack Cunningham"
format: pdf
editor: visual
---

2.1)

```{r,echo=FALSE}
occupation_data = matrix(c(.018,.002,.001,.001,.001,.035,
                           .112,.066,.018,.029,.031,.064,.094,.019,
                           .032, .008, .032, .032,.010, .043, .018,
                           .069,.084, .051, .13),nrow = 5, ncol = 5)

occupation_labels <- c("farm","operatives","craftsmen","sales",
                               "professional")

rownames(occupation_data) <- c("farm","operatives","craftsmen","sales",
                               "professional")
colnames(occupation_data) <- c("farm","operatives","craftsmen","sales",
                               "professional")

```

a\)

$$
p_{Y_{1}}(y_1)= \sum_{y_2 \in Y_2} p_{Y_1,Y_2}(y_1,y_2)
$$

Which leaves us with the marginal probability distribution of the father being:

```{r}
father_marginal_distribution <- rowSums(occupation_data)
names(father_marginal_distribution) <- occupation_labels
father_marginal_distribution
```

b\)

Similarly we have:

$$
p_{Y_2}(y_2)=\sum_{y_{1} \in Y_1}p_{Y_1,Y_2}(y_1,y_2)
$$

Which leaves us with the marginal probability distribution of the son being:

```{r}
son_marginal_distribution <- colSums(occupation_data)
names(son_marginal_distribution) <- occupation_labels
son_marginal_distribution
```

c\)

The conditional distribution of a son's occupation given that the father is a farmer is:

$$
p_{Y_2|Y_{1}}(y_2|y_1=\text{farmer})=\frac{Pr(\{Y_{1}=\text{farmer\} } \cap\{Y_{2}=y_2\})}{Pr(Y_{1} = \text{farmer})}
$$

```{r}
occupation_data["farm",]/father_marginal_distribution["farm"]
```

d\)

The conditional distribution of a father's occupation given that the son is a farmer is:

$$
p_{Y_1|Y_2}(y_1|y_{2}= \text{farmer})=\frac{Pr(\{Y_{1}=y_1 \}\cap \{ Y_{2}=\text{farmer}\}}{Pr(Y_2 = \text{farmer})}
$$

```{r}
occupation_data[,"farm"]/son_marginal_distribution["farm"]
```

2.2)

Since the two random variables are independent we have:

a\)

$$
E[a_1Y_1+a_2Y_2]=a_1 \mu_1+a_2\mu_2 \quad Var(a_1 Y_1+a_2 Y_2)=a_1^2 \sigma_1^2+a_2^2+\sigma_2^2
$$

b\)

$$
E[a_1Y_1-a_2Y_2]=a_1 \mu_1-a_2\mu_2 \quad Var(a_1 Y_1-a_2 Y_2)=a_1^2 \sigma_1^2+a_2^2+\sigma_2^2
$$

2.5)

a\)

|                |              |            |
|----------------|--------------|------------|
| $p_{X,Y}(x,y)$ | Green, Y = 1 | Red, Y = 0 |
| Heads, X = 1   | .2           | .3         |
| Tails, X = 0   | .3           | .2         |

b\)

$$
E[Y]=\sum_{y \in Y}\sum_{x \in X}y (p_{X,Y}(x,y))=1(.2)+0(.3)+1(.3)+(0)(.2)=.5
$$

The probability the ball is green is .5.

c\)

$$
Var(Y|X=0)=E[Y^2|X=0]-(E[Y|X=0])^2
$$

To find these expected values we must find the conditional probability distribution, $p_{Y|X}(y|x)$. We get this by applying the usual formula:

$$
p_{Y|X}(y|x)=\frac{Pr(\{Y=y\} \cap \{X =x \})}{Pr(X=x)}
$$

That leaves us with:

|                |              |            |
|----------------|--------------|------------|
| $p_{Y|X}(y|x)$ | Green, Y = 1 | Red, Y = 0 |
| Heads, X = 1   | .4           | .6         |
| Tails, X = 0   | .6           | .4         |

Using this we can calculate $Var(Y|X=0)$:

$$
Var(Y|X=0)=[1^2(.6)+0^2(.4)]-[1(.6)+0(.4)]^2=.6-.36=.24
$$

Similarly we calculate $Var(Y|X = 1)$:

$$
Var(Y|X = 1)=[1^2(.4)+0^2(.6)]-[1(.4)+0(.6)]^2=.4-.16=.24
$$

For $Var(Y)$ we have:

$$
Var(Y)=E[Y^2]-(E[Y])^2
$$

We already found $E[Y]$ being .5.

$$
E[Y^2]=\sum_{y \in Y} y^2 p_{X,Y}(x,y)=1^2(.2)+0^2(.3)+1^2(.3)+0^2(.2)=.5
$$

So we have:

$$
Var(Y)=.5-.5^2=.25
$$

Intuitively it makes sense that $Var(Y)$ has higher variance than the others, the coin flip determines the choice of urn to draw from. When we fix the result of the coin flip we have greater certainty about whether we will obtain a green or red ball.

d\)

We have:

$$
Pr(X =0|Y=1)=\frac{Pr(\{X=0\}\cap\{Y=1\})}{Pr(Y=1)}=\frac{.3}{.5}=.6
$$

Given the ball is green the probability the ball turned up tails is .6.

2.6)

We have $A \perp B|C$, so we know:

$$
P(A|B \cap C)=Pr(A|C)
$$

To show that this implies $A^c \perp B|C$ we use the law of total probability, $P(A|B \cap C)+P(A^c|B \cap C)=1$ on the left and $Pr(A|C)+Pr(A^c|C)=1$ on the right. So:

$$
1-Pr(A^c|B\cap C)=1-Pr(A^c|C)
$$

$$
Pr(A^c|B \cap C)=Pr(A^c |C)
$$

Thus showing that $A^c$ and $B$ are conditionally independent given C.

Similarly we can show that this implies $A \perp B^c|C$:

$$
Pr(B|A \cap C)=Pr(B|C)
$$

$$
1-Pr(B^c|A \cap C)=1-Pr(B^c|C)
$$

$$
Pr(B^c|A \cap C)=Pr(B^c|C)
$$

Thus showing that $A$ and $B^c$ are conditionally independent given C.

To show that this implies $A^c \perp B^c |C$ we need to show that $Pr(A^{c} \cap B^c|C)=Pr(A^c|C)Pr(B^c|C)$.

The first step relies on the De Morgan's rule:

$$
Pr(A^c \cap B^c|C)=Pr((A \cup B)^c|C)=1-Pr(A \cup B|C)
$$

$$
1-Pr( A \cup B|C)=1-(Pr(A|C)+Pr(B|C)-Pr(A \cap B|C))
$$

From there we use the fact that $A$ and $B$ are conditionally independent given $C$:

$$
= 1-(Pr(A|C)+Pr(B|C)-Pr(A|C)Pr(B|C))=(1-Pr(A|C))(1-Pr(B|C))=Pr(A^c|C)Pr(B^c|C)
$$

So we have shown $Pr(A^c \cap B^c|C)=Pr(A^c|C)Pr(B^c|C)$ which demonstrates that $A^c \perp B^c |C$ is true given $A\perp B|C$.

An example showing that $A \perp B|C$ holds but $A \perp B|C^c$ does not hold is as follows:
