---
title: "STAT 638 Homework 1"
author: "Jack Cunningham"
format: pdf
editor: visual
---

2.1)

```{r,echo=FALSE}
occupation_data = matrix(c(.018,.002,.001,.001,.001,.035,
                           .112,.066,.018,.029,.031,.064,.094,.019,
                           .032, .008, .032, .032,.010, .043, .018,
                           .069,.084, .051, .13),nrow = 5, ncol = 5)

occupation_labels <- c("farm","operatives","craftsmen","sales",
                               "professional")

rownames(occupation_data) <- c("farm","operatives","craftsmen","sales",
                               "professional")
colnames(occupation_data) <- c("farm","operatives","craftsmen","sales",
                               "professional")

```

a\)

$$
p_{Y_{1}}(y_1)= \sum_{y_2 \in Y_2} p_{Y_1,Y_2}(y_1,y_2)
$$

Which leaves us with the marginal probability distribution of the father being:

```{r}
father_marginal_distribution <- rowSums(occupation_data)
names(father_marginal_distribution) <- occupation_labels
father_marginal_distribution
```

b\)

Similarly we have:

$$
p_{Y_2}(y_2)=\sum_{y_{1} \in Y_1}p_{Y_1,Y_2}(y_1,y_2)
$$

Which leaves us with the marginal probability distribution of the son being:

```{r}
son_marginal_distribution <- colSums(occupation_data)
names(son_marginal_distribution) <- occupation_labels
son_marginal_distribution
```

c\)

The conditional distribution of a son's occupation given that the father is a farmer is:

$$
p_{Y_2|Y_{1}}(y_2|y_1=\text{farmer})=\frac{Pr(\{Y_{1}=\text{farmer\} } \cap\{Y_{2}=y_2\})}{Pr(Y_{1} = \text{farmer})}
$$

```{r}
occupation_data["farm",]/father_marginal_distribution["farm"]
```

d\)

The conditional distribution of a father's occupation given that the son is a farmer is:

$$
p_{Y_1|Y_2}(y_1|y_{2}= \text{farmer})=\frac{Pr(\{Y_{1}=y_1 \}\cap \{ Y_{2}=\text{farmer}\}}{Pr(Y_2 = \text{farmer})}
$$

```{r}
occupation_data[,"farm"]/son_marginal_distribution["farm"]
```

2.2)

Since the two random variables are independent we have:

a\)

$$
E[a_1Y_1+a_2Y_2]=a_1 \mu_1+a_2\mu_2 \quad Var(a_1 Y_1+a_2 Y_2)=a_1^2 \sigma_1^2+a_2^2+\sigma_2^2
$$

b\)

$$
E[a_1Y_1-a_2Y_2]=a_1 \mu_1-a_2\mu_2 \quad Var(a_1 Y_1-a_2 Y_2)=a_1^2 \sigma_1^2+a_2^2+\sigma_2^2
$$

2.3)

a\)

$p(x|y,z) =\frac{Pr(\{x=x_x\} \cap \{y=y_1\} \cap \{z=z_1\})}{Pr(y=y_1 \cap z=z_1)} =\frac{p(x,y,z)}{p(y,z)}$

To find $p(y,z)$ we sum over all x:

$p(y,z)=\sum_{x \in X} p(x,y,z)\propto g(y,z)h(z)\sum_{x \in X} f(x,z)$

Then we have:

$$
p(x|y,z)\propto\frac{p(x,y,z)}{g(y,z)h(z) \sum_{x \in X}f(x,z)} \propto\frac{f(x,z)g(y,z)h(z)}{g(y,z)h(z) \sum_{x \in}f(x,z)}\propto\frac{f(x,z)}{\sum_{x\in X}f(x,z)}
$$

Thus we have shown that $p(x|y,z)$ is a function of x and z.

b\)

$p(y|x,z) =\frac{p(x,y,z)}{p(x,z)}$

To find $p(x,z)$ we sum over all y:

$p(x,z)\propto\sum_{y \in Y} f(x,z)g(y,z)h(z) \propto f(x,z)h(z) \sum_{y \in Y}g(y,z)$

Then:

$$
p(y|x,z)\propto \frac{f(x,z)g(y,z)h(z)}{f(x,z)h(z)\sum_{y \in Y}g(y,z)} \propto \frac{g(y,z)}{\sum_{y \in Y}g(y,z)}
$$

So $p(y|x,z)$ is a function of y and z.

c\)

We start with the below, which is always true:

$$
Pr(X \cap Y|Z)=Pr(X|Z)Pr(Y|X \cap Z)
$$

Then, to show conditional independence all we need to show is that:

$$
Pr(Y|X \cap Z)=P(Y|Z)
$$

And since we have shown that $p(y|x,z)$ only depends on y and z we can say that $P(Y|X,Z)=P(Y|Z)$. Then we have:

$$
Pr(X\cap Y|Z)=P(X|Z)P(Y|Z)
$$Which demonstrates conditional independence between $X$ and $Y$ given $Z$ .

2.5)

a\)

|                |              |            |
|----------------|--------------|------------|
| $p_{X,Y}(x,y)$ | Green, Y = 1 | Red, Y = 0 |
| Heads, X = 1   | .2           | .3         |
| Tails, X = 0   | .3           | .2         |

b\)

$$
E[Y]=\sum_{y \in Y}\sum_{x \in X}y (p_{X,Y}(x,y))=1(.2)+0(.3)+1(.3)+(0)(.2)=.5
$$

The probability the ball is green is .5.

c\)

$$
Var(Y|X=0)=E[Y^2|X=0]-(E[Y|X=0])^2
$$

To find these expected values we must find the conditional probability distribution, $p_{Y|X}(y|x)$. We get this by applying the usual formula:

$$
p_{Y|X}(y|x)=\frac{Pr(\{Y=y\} \cap \{X =x \})}{Pr(X=x)}
$$

That leaves us with:

|                |              |            |
|----------------|--------------|------------|
| $p_{Y|X}(y|x)$ | Green, Y = 1 | Red, Y = 0 |
| Heads, X = 1   | .4           | .6         |
| Tails, X = 0   | .6           | .4         |

Using this we can calculate $Var(Y|X=0)$:

$$
Var(Y|X=0)=[1^2(.6)+0^2(.4)]-[1(.6)+0(.4)]^2=.6-.36=.24
$$

Similarly we calculate $Var(Y|X = 1)$:

$$
Var(Y|X = 1)=[1^2(.4)+0^2(.6)]-[1(.4)+0(.6)]^2=.4-.16=.24
$$

For $Var(Y)$ we have:

$$
Var(Y)=E[Y^2]-(E[Y])^2
$$

We already found $E[Y]$ being .5.

$$
E[Y^2]=\sum_{y \in Y} y^2 p_{X,Y}(x,y)=1^2(.2)+0^2(.3)+1^2(.3)+0^2(.2)=.5
$$

So we have:

$$
Var(Y)=.5-.5^2=.25
$$

Intuitively it makes sense that $Var(Y)$ has higher variance than the others, the coin flip determines the choice of urn to draw from. When we fix the result of the coin flip we have greater certainty about whether we will obtain a green or red ball.

d\)

We have:

$$
Pr(X =0|Y=1)=\frac{Pr(\{X=0\}\cap\{Y=1\})}{Pr(Y=1)}=\frac{.3}{.5}=.6
$$

Given the ball is green the probability the ball turned up tails is .6.

2.6)

We have $A \perp B|C$, so we know:

$$
P(A|B \cap C)=Pr(A|C)
$$

To show that this implies $A^c \perp B|C$ we use the law of total probability, $P(A|B \cap C)+P(A^c|B \cap C)=1$ on the left and $Pr(A|C)+Pr(A^c|C)=1$ on the right. So:

$$
1-Pr(A^c|B\cap C)=1-Pr(A^c|C)
$$

$$
Pr(A^c|B \cap C)=Pr(A^c |C)
$$

Thus showing that $A^c$ and $B$ are conditionally independent given C.

Similarly we can show that this implies $A \perp B^c|C$:

$$
Pr(B|A \cap C)=Pr(B|C)
$$

$$
1-Pr(B^c|A \cap C)=1-Pr(B^c|C)
$$

$$
Pr(B^c|A \cap C)=Pr(B^c|C)
$$

Thus showing that $A$ and $B^c$ are conditionally independent given C.

To show that this implies $A^c \perp B^c |C$ we need to show that $Pr(A^{c} \cap B^c|C)=Pr(A^c|C)Pr(B^c|C)$.

The first step relies on the De Morgan's rule:

$$
Pr(A^c \cap B^c|C)=Pr((A \cup B)^c|C)=1-Pr(A \cup B|C)
$$

$$
1-Pr( A \cup B|C)=1-(Pr(A|C)+Pr(B|C)-Pr(A \cap B|C))
$$

From there we use the fact that $A$ and $B$ are conditionally independent given $C$:

$$
= 1-(Pr(A|C)+Pr(B|C)-Pr(A|C)Pr(B|C))=(1-Pr(A|C))(1-Pr(B|C))=Pr(A^c|C)Pr(B^c|C)
$$

So we have shown $Pr(A^c \cap B^c|C)=Pr(A^c|C)Pr(B^c|C)$ which demonstrates that $A^c \perp B^c |C$ is true given $A\perp B|C$.

An example showing that $A \perp B|C$ holds but $A \perp B|C^c$ does not hold is as follows:

For $A \perp B|C$ to hold we have $P(A \cap B|C)=P(A|C)P(B|C)$.

Let's assign the probability of $A$ given $C$ is .25 and the probability of $B$ given $C$ is also .25. Lets also say that the probability of event $C$ is .5.

Then we have:

$$
\frac{P(A\cap B\cap C)}{P(C)}=P(A|C)P(B|C)=.125
$$

And:

$$
\frac{P(A \cap B \cap C^c)}{P(C^c)}=P(A \cap B |C^c)
$$

All we need is to show that in general:

$$
\frac{P(A \cap B \cap C^c)}{P(C^c)} \neq P(A|C^c)P(B|C^c)
$$

So if we choose $P(A|C^c)=.1$ and $P(B|C^c)=.2$ and $P(A \cap B|C^c)=.05$ and $P(C^c)=.5$ then:

$$
\frac{.05}{.5}\neq(.1)(.2)
$$

Thus showing that $A \perp B|C$ does not imply $A \perp B|C^c$.
