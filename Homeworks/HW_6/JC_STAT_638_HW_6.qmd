---
title: "JC STAT 638 HW 6"
author: "Jack Cunningham"
format: pdf
editor: visual
---

# 6.1

Reading Data:

```{r}
bach <- read.csv("bach.csv")
no_bach <- read.csv("nobach.csv")
```

a\)

Since $\theta_A$ and $\theta_B$ share the $\theta$ term they are dependent. A joint prior distribution is justified in cases where we cannot assume independence between parameters, such as here.

b\)

First we obtain the joint distribution:

$$
p(\theta,y_{a},y_{b},\gamma)=p(y_a|\theta_a)p(y_{b}|\theta_b)p(\gamma)p(\theta)
$$

Where:

$$
p(y_a|\theta_a)=\theta_a^{y_a}e^{-n_a \theta_a}=\theta^{y_a}e^{-n_a\theta}
$$

$$
p(y_b|\theta_b)=\theta_b^{y_b}e^{-n_b\theta_b}=\gamma^{y_b}\theta^{y_b}e^{-n_b(\theta \gamma)}
$$

$$
p(\gamma)=\gamma^{a_{\gamma}-1}e^{-(b_\gamma) \gamma}
$$

$$
p(\theta)=\theta^{a_{\theta}-1}e^{-(b_{\theta})\theta}
$$

Combined we have:

$$
p(\theta,y_{a},y_b,\gamma)=\theta^{y_a+y_n+a_{\theta}-1}\gamma^{ay-1}e^{-n_a\theta-n_b\theta-b_{\gamma}(\gamma)-b_{\theta}\theta}
$$

Then to obtain the form of the full conditional distribution of $\theta$ given $y_a,y_b,\gamma$ we treat all non-theta terms as constants and find the proportional density:

$$
p(\theta|y_a,y_b,\gamma) \propto \theta^{y_a+y_b+a_{\theta}-1} e^{-\theta(n_a+\gamma n_b+b_\theta)}
$$

Clearly the proportional density is $\text{gamma}(y_a+y_b+a_\theta,n_a+\gamma n_b+b_{\theta})$

c\)

We take the joint distribution and treat all non-gamma terms as constants to get the full conditional distribution of $\gamma$ given $y_a,y_b,\theta$:

$$
p(\gamma|y_a,y_b,\theta) \propto \gamma^{\alpha_{\gamma}+y_n-1}e^{-\gamma(n_b \theta+b_{\gamma})}
$$

Clearly the proportional density is $\text{gamma}(y_b+a_{y},n_b \theta+b \gamma)$

d\)

```{r}
set.seed(10)
#priors
a_theta = 2
b_theta = 1
a_gamma = b_gamma = c(8,16,32,64,128)
#data
y_a = sum(bach)
n_a = length(bach)
y_b = sum(no_bach)
n_b = length(no_bach)

#gibbs sampling
n_iter = 5000

theta <- numeric(n_iter)
gamma <- numeric(n_iter)
theta_diff <- numeric(5)

for (i in 1:length(a_gamma)){
  gamma[1] <- 1
  for(j in 2:n_iter){
    theta[j] <- rgamma(1, shape = y_a + y_b + a_theta, 
                       rate = n_a + n_b*gamma[j - 1] + b_theta)
    gamma[j] <- rgamma(1, shape = y_b + a_gamma[i],
                       rate = n_b*theta[j] + b_gamma[i])
  }
  theta_a <- theta
  theta_b <- theta * gamma
  theta_diff[i] <- mean(theta_b - theta_a)
    
}

```

```{r}
plot(x = a_gamma, y = theta_diff, type = "b",
     main = "Theta B - Theta A over different priors for gamma",
     xlab = "a Gamma , b Gamma",
     ylab = "Theta Difference")
```

As we make our prior on $\gamma$ stronger we can see that the difference we see between $\theta_B$ and $\theta_A$ tighten. This is because this prior is that of a relative rate $\theta_b/\theta_a$ equal to one, or in other words an assumption that the count of children has the same distribution regardless of whether the father holds a bachelor's degree.

# 6.3)

```{r}
raw_divorce_data <- read.table("divorce.dat")
df <- data.frame(age_diff = raw_divorce_data$V1, divorce = raw_divorce_data$V2)
head(df)
```

a\)

We only need information about $z,x$ to have all the information we need for $p(\beta|y,x,z,c)$ so we use the Bayes rule.

$$p(\beta|y,x,z,c)=p(\beta|z,x)=p(z|\beta,x)p(\beta)$$

We know that $\beta \sim \text{normal}(0,\tau^2_{B})$, so:

$$
p(\beta)\propto e^{-\beta^2/2\tau_b^2}
$$

We have:

$$
Z_i=\beta x_i+\epsilon_i
$$

We also know that $\epsilon_i \sim \text{i.i.d normal(0,1)}$, so:

$$
Z_i \sim N(\beta x_i,1)
$$

Then:

$$
p(z_i|\beta,x_i) \propto \exp[-\frac{1}{2} (z_i-\beta x_i)^2/1]
$$

Then over all $z_i$ we have:

$$
p(z|\beta,x) \propto \exp[-\frac{1}{2}\sum_{i=1}^n (z_i-\beta x_i)^2]
$$

So we end up with:

$$
p(\beta|y,x,z,c) \propto \exp[-\frac{1}{2}\sum_{i=1}^n(z_i-\beta{x_i})^2 +\frac{\beta^2}{\tau_b^2}]
$$

After some more algebra we get to:

$$
p(\beta|y,x,z,c) \propto \exp[-\frac{1}{2}(\beta^2(\sum x_i^2 + \frac{1}{\tau_b^2})-2\beta \sum x_i z_i+ \sum z_i^2]
$$

This reduces to:

$$
p(\beta |z,x) \propto \exp[-\frac{1}{2 V_\beta}(\beta -\mu_\beta)^2]
$$

where $V_\beta=\frac{1}{\sum_ix_i^2+\frac{1}{\tau_\beta^2}},\mu_\beta=V_\beta \sum_{i} x_i z_i$ .

So:

$$
\beta|x,y,z,c \sim N(\mu_B, V_B)
$$

b\)

We have:

$$
p(c|y,x,z,\beta)=p(y|x,z,\beta ,c)p(c)
$$

Since we are assuming $c \sim N(0,\tau_c^2)$ we have:

$$
p(c) \propto \exp[-c^2/2\tau_c^2]
$$

Then we have to find the form of $p(y|x,z,\beta,c)$.

We have $Y_i=\delta_{(c,\infty)}(Z_i)$ , which is an indicator function where if $Z_i>c$ we have $Y_i=1$ and if $Z_i \leq c$ we have $Y_i=0$. So:

$$
Y_i=1 \{Z_i > c\}
$$

Then, since we know each $Z_i$, we have some information about what $c$ is.

For instance, if a $Y_i=1$ we know that $Z_i > c$ or equivalently $c < Z_i$.

Similarly, if a $Y_i=0$ we know that $Z_i<c$ or equivalently $c>Z_i$.

Then, for all $Z_i$ we know that:

$$
c_{\min}= \max[Z_i|Y_i=0], \quad c_{\max}=\min [Z_i|Y_i =1]
$$ Then that leaves us with:

$$
p(c|y,x,z,\beta)=\exp[-c^2/2\tau^2_c] \times1\{c_{\min} <c<c_{\max}\}
$$

Which we can see is proportional to the normal density, but $c$ is constrained in the interval $(c_{\min},c_{\max})$.

The same idea carries for $p(z_i|y,x,z_{-i},\beta,c)$:

If we know about $y_i$ we have information that $z_i$ is above or below c. So:

$$
p(z_i|y,x,z_{-i},\beta,c) \propto \begin{cases} N(\beta x_i,1) & z_i>c  \quad y_i=1\\
N(\beta x_i,1) & z_i \leq c  \quad y_i = 0 \end{cases}
$$

c\)

```{r,cache=TRUE}
set.seed(2)
n <- dim(df)[1]
x <- df$age_diff
y <- df$divorce

#Priors
tau2_beta <- 16
tau2_c <- 16
tau_beta <- sqrt(tau2_beta)
tau_c <- sqrt(tau2_c)

#Initalization
beta <- 0
c <- 0
z <- numeric(n)
n_iter <- 20000
beta_store <- numeric(n_iter)
c_store <- numeric(n_iter)

# Gibbs Sample
for (iter in 1:n_iter) {
  # Z sampling
  mu <- beta * x
  for (i in 1:n) {
    if (y[i] == 1) {
      a <- (c - mu[i]) / 1
      u <- runif(1, pnorm(a), 1)
      z[i] <- mu[i] + qnorm(u)
    } else {
      b <- (c - mu[i]) / 1
      u <- runif(1, 0, pnorm(b))
      z[i] <- mu[i] + qnorm(u)
    }
  }

  # Beta sampling
  V_beta <- 1 / (sum(x^2) + 1 / tau2_beta)
  mu_beta <- V_beta * sum(x * z)
  beta <- rnorm(1, mu_beta, sqrt(V_beta))

  # C sampling
  a <- if (any(y == 0)) max(z[y == 0]) else -Inf
  b <- if (any(y == 1)) min(z[y == 1]) else Inf
  lower <- pnorm((a - 0) / tau_c)
  upper <- pnorm((b - 0) / tau_c)
  u <- runif(1, lower, upper)
  c <- 0 + tau_c * qnorm(u)

  # storing
  beta_store[iter] <- beta
  c_store[iter] <- c
}

```

```{r,fig.height = 10, fig.width = 8}
par(mfrow = c(2,1))
acf(beta_store, main = "ACF of beta")
acf(c_store, main = "ACF of c")

```

We can see that both $\beta$ and $c$ seem to have poor mixing. The autocorrelation between samples close to each other is high and it takes about 40 lags for the autocorrelation to drop near zero. This would indicate that we may need to use more iterations to get good precision.

d\)

```{r}
cat("Posterior mean of beta:", mean(beta_store), "\n")
cat("95% credible interval for beta:", quantile(beta_store, c(0.025, 0.975)), "\n")
cat("Pr(beta > 0 | data):", mean(beta_store > 0), "\n")
```
